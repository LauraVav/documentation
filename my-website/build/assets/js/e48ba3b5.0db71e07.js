"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8696],{3905:function(e,t,a){a.d(t,{Zo:function(){return s},kt:function(){return d}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var u=n.createContext({}),c=function(e){var t=n.useContext(u),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},s=function(e){var t=c(e.components);return n.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},y=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,u=e.parentName,s=i(e,["components","mdxType","originalType","parentName"]),y=c(a),d=r,m=y["".concat(u,".").concat(d)]||y[d]||p[d]||o;return a?n.createElement(m,l(l({ref:t},s),{},{components:a})):n.createElement(m,l({ref:t},s))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,l=new Array(o);l[0]=y;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i.mdxType="string"==typeof e?e:r,l[1]=i;for(var c=2;c<o;c++)l[c]=a[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}y.displayName="MDXCreateElement"},4529:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return i},contentTitle:function(){return u},metadata:function(){return c},toc:function(){return s},default:function(){return y}});var n=a(7462),r=a(3366),o=(a(7294),a(3905)),l=["components"],i={sidebar_position:1},u="Vocabulary",c={unversionedId:"Tutorials/Python SDK/vocabulary",id:"Tutorials/Python SDK/vocabulary",isDocsHomePage:!1,title:"Vocabulary",description:"Video",source:"@site/docs/Tutorials/Python SDK/vocabulary.md",sourceDirName:"Tutorials/Python SDK",slug:"/Tutorials/Python SDK/vocabulary",permalink:"/docs/Tutorials/Python SDK/vocabulary",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"NER",permalink:"/docs/Tutorials/API/ner"},next:{title:"Word Count",permalink:"/docs/Tutorials/Python SDK/word-count"}},s=[{value:"Video",id:"video",children:[],level:2},{value:"What is the vocabulary tokenization?",id:"what-is-the-vocabulary-tokenization",children:[],level:2},{value:"Importing the library &amp; your personal API key",id:"importing-the-library--your-personal-api-key",children:[],level:2},{value:"Adding your document",id:"adding-your-document",children:[],level:2},{value:"Extracting vocabulary",id:"extracting-vocabulary",children:[],level:2},{value:"Extracting more details",id:"extracting-more-details",children:[],level:2},{value:"Saving your results",id:"saving-your-results",children:[],level:2},{value:"Code set",id:"code-set",children:[],level:2}],p={toc:s};function y(e){var t=e.components,i=(0,r.Z)(e,l);return(0,o.kt)("wrapper",(0,n.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"vocabulary"},"Vocabulary"),(0,o.kt)("h2",{id:"video"},"Video"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"vocabulary",src:a(8988).Z})),(0,o.kt)("h2",{id:"what-is-the-vocabulary-tokenization"},"What is the vocabulary tokenization?"),(0,o.kt)("p",null,"Vocabulary tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. Lettria enables you to quickly and easily extract the vocabulary as well as the part of speech to enable you to easily filter and analyze your document."),(0,o.kt)("p",null,"In order to extract the vocabulary from your document you can use the TextChunk class Vocabulary tool."),(0,o.kt)("h2",{id:"importing-the-library--your-personal-api-key"},"Importing the library & your personal API key"),(0,o.kt)("p",null,"After you've installed the Lettria package on Python you'll need to import the library."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import lettria\n")),(0,o.kt)("p",null,"Next you are going to need to include your personal API key which can be found"),(0,o.kt)("p",null,"via the Lettria platform in the dashboard."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"api_key = 'your personal API key'\nnlp = lettria.NLP(api_key)\n")),(0,o.kt)("h2",{id:"adding-your-document"},"Adding your document"),(0,o.kt)("p",null,"Now you will need to open your saved document. Be sure to add the name of"),(0,o.kt)("p",null,"\u2018your file\u2019 since it may differ from the name of my example file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'with open("example.txt", "r") as f:\n    example_data = f.readlines()\n')),(0,o.kt)("p",null,"Next, add the document to the NLP."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"nlp.add_document(example_data)\n")),(0,o.kt)("h2",{id:"extracting-vocabulary"},"Extracting vocabulary"),(0,o.kt)("p",null,"In order to extract the vocabulary of the document use the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"vocabulary = nlp.vocabulary\nvocabulary(filter_pos = None, lemma=False)\n")),(0,o.kt)("p",null,"In the results you will have a list of the vocabulary, part of speech and lemma."),(0,o.kt)("h2",{id:"extracting-more-details"},"Extracting more details"),(0,o.kt)("p",null,"If you would like to filter out parts of speech such as \u2018nouns\u2019 you can change the filter criteria."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"vocabulary = nlp.vocabulary\nvocabulary(filter_pos = 'N', lemma=False)\n")),(0,o.kt)("p",null,"Now you can see within the results all the vocabulary falling under \u2018noun\u2019 has been filtered out."),(0,o.kt)("h2",{id:"saving-your-results"},"Saving your results"),(0,o.kt)("p",null,"In order to save your results you can use the following command."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"nlp.save_results(\u2018example_results')\n")),(0,o.kt)("p",null,"And a json file with your results that can be used for further analysis will be saved."),(0,o.kt)("h2",{id:"code-set"},"Code set"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import lettria\n\napi_key = 'your personal API key'\nnlp = lettria.NLP(api_key)\n\nwith open(\"example.txt\", \"r\") as f:\n    example_data = f.readlines()\n\nnlp.add_document(example_data)\n\nvocabulary = nlp.vocabulary\nvocabulary(filter_pos = None, lemma=False)\n\nvocabulary = nlp.vocabulary\nvocabulary(filter_pos = 'N', lemma=False)\n\nnlp.save_results(\u2018example_results')\n")))}y.isMDXComponent=!0},8988:function(e,t,a){t.Z=a.p+"assets/images/vocabulary-CS-33801c9a48ae8d361a2c01883da266a1.png"}}]);