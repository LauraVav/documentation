"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3088],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=o.createContext({}),u=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=u(e.components);return o.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),d=u(n),m=r,h=d["".concat(s,".").concat(m)]||d[m]||c[m]||a;return n?o.createElement(h,i(i({ref:t},p),{},{components:n})):o.createElement(h,i({ref:t},p))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var u=2;u<a;u++)i[u]=n[u];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7346:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return u},toc:function(){return p},default:function(){return d}});var o=n(7462),r=n(3366),a=(n(7294),n(3905)),i=["components"],l={sidebar_position:5},s="Tokenizer",u={unversionedId:"Tutorials/tokenizer",id:"Tutorials/tokenizer",isDocsHomePage:!1,title:"Tokenizer",description:"What\u2019s a tokenizer?",source:"@site/docs/Tutorials/tokenizer.md",sourceDirName:"Tutorials",slug:"/Tutorials/tokenizer",permalink:"/docs/Tutorials/tokenizer",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Language Recognition",permalink:"/docs/Tutorials/language-recognition"}},p=[{value:"What\u2019s a tokenizer?",id:"whats-a-tokenizer",children:[],level:2},{value:"What does it do? How does it work?",id:"what-does-it-do-how-does-it-work",children:[],level:2},{value:"Tutorial",id:"tutorial",children:[],level:2},{value:"Code set",id:"code-set",children:[],level:2}],c={toc:p};function d(e){var t=e.components,n=(0,r.Z)(e,i);return(0,a.kt)("wrapper",(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"tokenizer"},"Tokenizer"),(0,a.kt)("h2",{id:"whats-a-tokenizer"},"What\u2019s a tokenizer?"),(0,a.kt)("p",null,"A tokenizer is a tool founded on an algorithm based on a set of rules or on the learning of a manually-labelled corpus. ",(0,a.kt)("strong",{parentName:"p"},"It allows text to be broken down into words.")," It\u2019s a morphological analysis."),(0,a.kt)("h2",{id:"what-does-it-do-how-does-it-work"},"What does it do? How does it work?"),(0,a.kt)("p",null,"Tokenization is a type of segmentation that ",(0,a.kt)("strong",{parentName:"p"},"breaks down a sentence into multiple elements.")),(0,a.kt)("p",null,"In Natural Language Processing (NLP), tokenization is a part of the standardization process. It segments the input text into manipulable linguistic units like words, punctuation, numbers, alphanumeric data, etc. Each element corresponds to a ",(0,a.kt)("strong",{parentName:"p"},"token")," that will be used for analysis."),(0,a.kt)("p",null,"The goal of this process is to separate the basic units of a text that will then lend themselves to in-depth analysis. One might think that it would be sufficient to detect the spaces between words, but it\u2019s not always that simple."),(0,a.kt)("p",null,"What to do with hyphens? Where to cut when there are apostrophes? What to make of nominal phrases like \u201cclothes iron\u201d that are composed of several words but designate a single entity?"),(0,a.kt)("p",null,"So, if we accept that the word \u201cgo-to-market\u201d is composed of 3 distinct lexical units, (go/to/market), it is more difficult to determine the number of words. One must ",(0,a.kt)("strong",{parentName:"p"},"define that it\u2019s a token")," before pursuing the analysis."),(0,a.kt)("p",null,"Efficient pattern recognition needs to adapt itself to the conveyed realities of the text subject to analysis. A tokenizer is completely dependent on the language we\u2019re working on."),(0,a.kt)("h2",{id:"tutorial"},"Tutorial"),(0,a.kt)("p",null,"Tokenization is a type of segmentation that ",(0,a.kt)("strong",{parentName:"p"},"breaks down a sentence into multiple elements.")," The goal of this process is to separate the basic units of a text that will then lend themselves to in-depth analysis."),(0,a.kt)("p",null,"If you want to extract tokens from your document you'll need to have your document saved on your computer."),(0,a.kt)("p",null,"After you've installed the Lettria package on Python you'll need to import the library."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import lettria\n")),(0,a.kt)("p",null,"Next you are going to need to include your personal API key which can be found"),(0,a.kt)("p",null,"via the Lettria platform in the dashboard."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"api_key = 'your personal API key'\nnlp = lettria.NLP(api_key)\n")),(0,a.kt)("p",null,"Now you will need to open your saved document. Be sure to add the name of"),(0,a.kt)("p",null,"\u2018your file\u2019 since it may differ from the name of the example file."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'with open("example.txt", "r") as f:\n    example_data = f.readlines()\n')),(0,a.kt)("p",null,"Next add your document to the NLP."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"nlp.add_document(example_data)\n")),(0,a.kt)("p",null,"Next you will need to call the nlp and print all tokens from the document."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"for t in nlp.tokens:\n    print(t.token)\n")),(0,a.kt)("p",null,"In order to save your results you can use the following command."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"nlp.save_results(\u2018example_results')\n")),(0,a.kt)("p",null,"And a json file with your results that can be used for further analysis will be saved."),(0,a.kt)("h2",{id:"code-set"},"Code set"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import lettria\n\napi_key = \'your personal API key\'\nnlp = lettria.NLP(api_key)\n\nwith open("example.txt", "r") as f:\n    example_data = f.readlines()\n\nnlp.add_document(example_data)\n\nfor t in nlp.tokens:\n    print(t.token)\n\nnlp.save_results(\u2018example_results\')\n')))}d.isMDXComponent=!0}}]);